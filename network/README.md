# Artificial Neural Network Models

## ANN_Layer

All modelling is based off the ANN_Layer class. This class models a singular
layer in an artificial neural network. 

### Basic Usage

Start by creating an ANN_Layer. The basic constructor provided takes the input
and output counts. 

    ANN_Layer layer = new ANN_Layer(5, 3);

By default, the ANN_Layer uses the identity function for activation. To specify
a different activation function, provide either a DifferentiableScalarFunction
or a DifferentiableVectorFunction as defined by the interfaces in the function
directory. 

    layer.set_activation_function(SoftplusFunction.implementation);
    layer.set_activation_function(SoftmaxFunction.implementation);

To obtain the output of the network, use the pass method. 

    double[] output = layer.pass(3, 1, 4, 1, 5);

Extra notes:
 - Weights and biases are initialized using the normal distribution. Currently,
   there is an extra static method that sets all weights and biases to a given
   value. 
 - Because the activation function can be scalar or vector, the function itself
   is stored as an object. Helper functions activation_pass and
   activation_derivative determine what type of activation function the layer is
   using and passes values through the function safely (i.e. pass by copy). 

### Backpropogation

Backpropogation works by passing the derivative of the cost function with
respect to the layer outputs. The ANN_Layer stores a history of all inputs,
intermediates, and outputs presented to and generated by the layer. The history
is erased when weights and biases are updated, or when the
clear_activation_history method is called. 

To backpropogate through the network, first feed an input forward. 

    double[] output = layer.pass(2, 7, 1, 8, 3);

Then based on the output, calculate the derivative of the loss function. Pass
the derivative to the layer. **This does not update weights and biases yet**.

    double[] dCdy = ...        // Calculate separately
    double[] dCdx = layer.load_derivative(dCdy);

The derivative of the cost function with respect to the layer outputs is passed
to load_derivative. The load_derivative function then returns the derivative
with respect to the layer inputs, allowing for further backpropogation. 

Derivative must match with an input. The load_derivative will always use the
most recent input in the history to do calculations. Once calculations are
complete, load_derivative will remove the most recent record in the history.
The next call to load_derivative will then match with the next-most recent
input passed through the network.

    double[] output_1 = network.pass(input_1);
    double[] output_2 = network.pass(input_2);
    double[] dCdx_2 = network.load_derivative(dCdy_2); // Matches with input_2
    double[] dCdx_1 = network.load_derivative(dCdy_1); // Matches with input_1

**It is important to note that load_derivative does not update weights and
biases.** To apply the derivative, use the respective method. 

    network.apply_derivatives();

This will perform batch gradient decent using all derivatives currently loaded
into the layer. Derivatives are combined through addition without scaling.
Applying the derivatives will also clear the activation history. 

Reasonings:
 - The main reason I implemented ANN_Layer this way is because of recurrent
   neural networks. As explained later, recurrent networks work by passing
   inputs to the same ANN_Layer multiple times. From a feed-forward perspective,
   the ANN_Layer would seem to remember past inputs. 
 - For backpropogation through time, one simply calculates dCdy for the current
   input and loads the derivative. Then layer will then provide dCdx for the
   layer, which can then be loaded into the network again, this time with the
   previous input. 
 - As a side product, this allows for minibatch gradient decent. 

Extra notes:
 - A static field stores the default learning rate for all ANN_Layers. Use the
   set_learning_rate method to specify learning rates for each layer. 
 - The learning rate for each layer decreases over time, proportional to the
   inverse square root. Time is dictated by how many times weights and biases
   are updated. Changing the learning rate will reset the counter. Use method
   get_learning_rate to determine the decayed learning rate. 

