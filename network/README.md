# Artificial Neural Network Models

## ANN_Layer

All modelling is based off the ANN_Layer class. This class models a singular
layer in an artificial neural network. 

### Basic Usage

Start by creating an ANN_Layer. The basic constructor provided takes the input
and output counts. 

    ANN_Layer layer = new ANN_Layer(5, 3);

By default, the ANN_Layer uses the identity function for activation. To specify
a different activation function, provide either a DifferentiableScalarFunction
or a DifferentiableVectorFunction as defined by the interfaces in the function
directory. 

    layer.set_activation_function(SoftplusFunction.implementation);
    layer.set_activation_function(SoftmaxFunction.implementation);

To obtain the output of the network, use the pass method. 

    double[] output = layer.pass(3, 1, 4, 1, 5);

Extra notes:
 - Weights and biases are initialized using the normal distribution. Currently,
   there is an extra static method that sets all weights and biases to a given
   value. 
 - Because the activation function can be scalar or vector, the function itself
   is stored as an object. Helper functions activation_pass and
   activation_derivative determine what type of activation function the layer is
   using and passes values through the function safely (i.e. pass by copy). 

### Backpropogation

Backpropogation works by passing the derivative of the cost function with
respect to the layer outputs. The ANN_Layer stores a history of all inputs,
intermediates, and outputs presented to and generated by the layer. The history
is erased when weights and biases are updated, or when the
clear_activation_history method is called. 

To backpropogate through the network, first feed an input forward. 

    double[] output = layer.pass(2, 7, 1, 8, 3);

Then based on the output, calculate the derivative of the loss function. Pass
the derivative to the layer. **This does not update weights and biases yet**.

    double[] dCdy = ...        // Calculate separately
    double[] dCdx = layer.load_derivative(dCdy);

The derivative of the cost function with respect to the layer outputs is passed
to load_derivative. The load_derivative function then returns the derivative
with respect to the layer inputs, allowing for further backpropogation. 

Derivative must match with an input. The load_derivative will always use the
most recent input in the history to do calculations. Once calculations are
complete, load_derivative will remove the most recent record in the history.
The next call to load_derivative will then match with the next-most recent
input passed through the network.

    double[] output_1 = network.pass(input_1);
    double[] output_2 = network.pass(input_2);
    double[] dCdx_2 = network.load_derivative(dCdy_2); // Matches with input_2
    double[] dCdx_1 = network.load_derivative(dCdy_1); // Matches with input_1

**It is important to note that load_derivative does not update weights and
biases unless it clears the history.** To apply the derivative, use the
respective method. 

    network.apply_derivatives();

This will perform batch gradient decent using all derivatives currently loaded
into the layer. Derivatives are combined through addition without scaling. The
derivatives will be automatically applied when loading a derivative if the
activation history is fully cleared by the load. Applying the derivatives will
also clear the activation history. 

Reasonings:
 - The biggest reason I implemented ANN_Layer this way is because of recurrent
   neural networks. As explained later, recurrent networks work by passing
   inputs to the same ANN_Layer multiple times. 
 - For backpropogation through time, one simply calculates dCdy for the current
   input and loads the derivative. Then layer will then provide dCdx for the
   layer, which can then be loaded into the network again, this time with the
   previous input. 
 - As a side product, this allows for minibatch gradient decent, although less
   efficient to use. 

Extra notes:
 - A static field stores the default learning rate for all ANN_Layers. Use the
   set_learning_rate method to specify learning rates for each layer. 
 - The learning rate for each layer decreases over time, proportional to the
   inverse square root. Time is dictated by how many times weights and biases
   are updated. Changing the learning rate will reset the counter. Use method
   get_learning_rate to determine the decayed learning rate. 

## BaseNetwork

An ANN_Layer by itself models a fully connected artificial neural network with
no hidden layers. Then to add hidden layers, the BaseNetwork class packages
multiple ANN_Layers together. 

### Basic Usage

To create a BaseNetwork, one must specify the number of nodes in each layer. 

    BaseNetwork network = new BaseNetwork(784, 64, 64, 10);

This example network has 784 input neurons and 10 output neurons, along with two
hidden layers with 64 neurons each. 

Recall that ANN_Layers are initialized to use an identity activation function. 
Activation functions can be specified either for the whole network, or specific
layers. Layers are zero-indexed, with layer 0 connecting the input layer to the
first hidden layer. 

    // Switches layers 1 and 2 to the softplus activation function
    network.set_activation_function(SoftplusFunction.implementation, 1, 2);

    // Sets all layers to the softmax activation function
    network.set_activation_function(SoftmaxFunction.implementation);

The same syntax can be used to set the learning rates of each layer through the
set_learning_rate function. 

The pass and load_derivative functions chains through all ANN_Layer instances,
exactly how one would expect from a neural network. 

    network.pass(3, 1, 4, 1, 5, 9, 2, 6, ...);
    network.load_derivative(2, 7, 1, 8, 2, 8, ...);

Other functions exist to call the respective functions in all ANN_Layer
instances. These functions include the following. 
 - clear_activation_history
 - apply_derivatives

Extra notes:
 - The BaseLayer makes no effort to store histories because all records are
   stored within the ANN_Layer instances. Although the output of one layer is
   the input to the next layer, each layer stores its own copy of the history. 
 - The load_derivative function returns the derivative of the cost function with
   respect to the input to the full network. This may be useful for more complex
   network models in future implementations. 

